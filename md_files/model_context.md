Проблема, которую вы описываете, фундаментальная для всех LLM: модель видит только текущий контекст (messages), но не имеет "памяти состояния" между примерами обучения. На 9 ноября 2025 года **лучший масштабируемый и гибкий подход** — это использование **multi-turn dialogue с полной историей + структурным усилением контекста**, а не хардкод-проверок.[1][2][3]

## Рекомендуемое решение: Multi-Turn Context Retention через правильную структуру данных

### 1. **Полная история диалога в каждом примере обучения**

Вместо того чтобы обучать модель на изолированных парах "вопрос→ответ", нужно **всегда включать полную цепочку предыдущих сообщений**:[4][2][1]

```python
# ПЛОХО (текущий подход — каждый пример изолирован)
{
    "messages": [
        {"role": "system", "content": "Ты продавец..."},
        {"role": "user", "content": "не брат, пока просто нужно было узнать"},
        {"role": "assistant", "content": "Ассаляму алейкум..."}  # ❌ Модель не видит, что уже здоровалась
    ]
}

# ХОРОШО (полная история с начала диалога)
{
    "messages": [
        {"role": "system", "content": "Ты продавец..."},
        {"role": "user", "content": "Привет, есть коллаген?"},
        {"role": "assistant", "content": "Ассаляму алейкум ва рахматуллахи ва баракатух, брат!..."},
        {"role": "user", "content": "Как оформить заказ?"},
        {"role": "assistant", "content": "Напишите ФИО, номер телефона..."},  # ✅ Без повторного приветствия
        {"role": "user", "content": "не брат, пока просто нужно было узнать"},
        {"role": "assistant", "content": "от души, тогда до связи"}  # ✅ Модель видит всю историю
    ]
}
```

### 2. **Препроцессинг данных: формирование накопительной истории**

Модифицируйте pipeline подготовки JSONL так, чтобы для каждого последующего сообщения в диалоге сохранялась **вся предыдущая история**:[2][1]

```python
def prepare_multiturn_dataset(raw_conversations):
    """
    Превращает линейные диалоги в multi-turn примеры с накопительной историей
    """
    training_examples = []
    
    for conversation in raw_conversations:
        system_prompt = {"role": "system", "content": "Ты продавец..."}
        history = [system_prompt]
        
        # Для каждой пары user-assistant создаём отдельный пример
        for turn in conversation:
            user_msg = {"role": "user", "content": turn["user"]}
            assistant_msg = {"role": "assistant", "content": turn["assistant"]}
            
            # Добавляем user в историю
            history.append(user_msg)
            
            # Создаём обучающий пример с полной историей до этого момента
            training_examples.append({
                "messages": history.copy() + [assistant_msg]  # История + текущий ответ
            })
            
            # Добавляем assistant в историю для следующего шага
            history.append(assistant_msg)
    
    return training_examples
```

### 3. **Multi-level attention для длинных контекстов**

Если диалоги становятся очень длинными (>4K токенов), используйте **иерархическую память**:[5][3]

- **Рабочая память** (последние 5-10 сообщений) — всегда в полном виде
- **Эпизодическая память** (ранняя часть диалога) — сжатая через summarization или ключевые факты

```python
def compress_long_history(messages, max_recent=10):
    """
    Сжимает длинную историю, сохраняя детали последних сообщений
    """
    if len(messages) <= max_recent:
        return messages
    
    # Первое сообщение (system) + сжатая середина + последние N
    system = messages[0]
    middle = messages[1:-max_recent]
    recent = messages[-max_recent:]
    
    # Сжимаем середину через суммаризацию
    summary = summarize_conversation(middle)  # LLM или rule-based
    summary_msg = {
        "role": "system", 
        "content": f"История диалога: {summary}"
    }
    
    return [system, summary_msg] + recent
```

### 4. **Reinforcement Learning with Human Feedback (RLHF) для контекстного поведения**

После базового fine-tuning добавьте **RL-фазу**, где модель получает reward за правильное использование контекста:[6][1]

- **Positive reward**: не повторяет приветствие, если оно было в истории
- **Negative reward**: повторяет информацию или игнорирует контекст

Это можно сделать через DPO (Direct Preference Optimization) — более простой аналог RLHF:[7]

```python
# Пример пары для DPO
{
    "prompt": [полная история с приветствием],
    "chosen": "от души, тогда до связи",  # ✅ Правильно — без повтора
    "rejected": "Ассаляму алейкум... [повтор приветствия]"  # ❌ Неправильно
}
```

***

## Итоговая архитектура (актуальная на 9 ноября 2025)

1. **Датасет**: Каждый обучающий пример содержит полную накопительную историю диалога от начала[1][2]
2. **Коллатор**: Передаёт всю историю в processor (GLM/Qwen) с правильной маркировкой labels[1]
3. **Длинный контекст**: Для диалогов >4K токенов — иерархическое сжатие[3]
4. **Post-training**: DPO для усиления контекстной когерентности[7][6]
5. **Inference**: CloneResponder хранит полную историю текущей сессии и передаёт её в модель

***

## Почему это работает

- Модель **видит паттерны**: "если в истории есть приветствие → не повторять"[4][1]
- **Масштабируемо**: работает для любых повторений (не только приветствия), так как модель учится на контексте, а не на хардкод-правилах[3][2]
- **Актуально**: Это стандарт 2025 года для dialogue state tracking и multi-turn reasoning[8][6][1]

Эта архитектура используется в современных production-системах (ChatGPT, Claude, Gemini) и позволяет модели самостоятельно понимать состояние диалога без явных маркеров.[5][2][3]

